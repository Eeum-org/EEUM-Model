import logging
import os
import shutil
import torch
import torch.nn as nn
import torch.nn.functional as F
import multiprocessing as mp
import time
import torch.optim as optim
from lion_pytorch import Lion
from tqdm.auto import tqdm
from lib.config import get_cfg
from lib.dataset import build_data_loader
from torch.utils.tensorboard import SummaryWriter
from lib.engine import default_argument_parser, default_setup
from lib.model.sign_model_keypoint import KeypointTransformer
from lib.solver import build_lr_scheduler, build_optimizer
from lib.utils import AverageMeter, clean_ksl, wer_list
import sys
best_wer = 100
def check_weight_initialization(model):
    """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ìƒíƒœ í™•ì¸"""
    print("=== ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ìƒíƒœ ===")
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            # ê°€ì¤‘ì¹˜ í†µê³„
            mean_val = param.data.mean().item()
            std_val = param.data.std().item()
            min_val = param.data.min().item()
            max_val = param.data.max().item()
            
            print(f"{name}:")
            print(f"  í˜•íƒœ: {param.shape}")
            print(f"  í‰ê· : {mean_val:.6f}, í‘œì¤€í¸ì°¨: {std_val:.6f}")
            print(f"  ë²”ìœ„: [{min_val:.6f}, {max_val:.6f}]")
            
            # ì´ˆê¸°í™” ë¬¸ì œ ê°ì§€
            if std_val < 1e-6:
                print(f"  âš ï¸ í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ ì‘ìŒ (ê±°ì˜ 0)")
            if abs(mean_val) > 1.0:
                print(f"  âš ï¸ í‰ê· ê°’ì´ ë„ˆë¬´ í¼")
            if std_val > 2.0:
                print(f"  âš ï¸ í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ í¼")
            
            # ëª¨ë“  ê°’ì´ ê°™ì€ì§€ í™•ì¸
            if param.data.unique().numel() == 1:
                print(f"  âš ï¸ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ë™ì¼í•¨!")
            
            print()
def check_gradient_flow(model, loss):
    """ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ í™•ì¸"""
    print("=== ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ í™•ì¸ ===")
    
    # ì—­ì „íŒŒ ìˆ˜í–‰
    loss.backward(retain_graph=True)
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.data.norm(2).item()
            print(f"{name}: grad_norm = {grad_norm:.6f}")
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ë¬¸ì œ ê°ì§€
            if grad_norm < 1e-7:
                print(f"  âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë„ˆë¬´ ì‘ìŒ (vanishing)")
            elif grad_norm > 100:
                print(f"  âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë„ˆë¬´ í¼ (exploding)")
            elif torch.isnan(param.grad).any():
                print(f"  âš ï¸ ê·¸ë˜ë””ì–¸íŠ¸ì— NaN ì¡´ì¬")
        else:
            print(f"{name}: ê·¸ë˜ë””ì–¸íŠ¸ ì—†ìŒ!")
def debug_forward_pass(model, input_tensor):
    """Forward pass ë‹¨ê³„ë³„ ë””ë²„ê¹…"""
    print("=== Forward Pass ë””ë²„ê¹… ===")
    
    x = input_tensor
    print(f"ì…ë ¥: {x.shape}, mean={x.mean():.4f}, std={x.std():.4f}")
    
    # ê° ëª¨ë“ˆë³„ë¡œ ì¶œë ¥ í™•ì¸
    for i, (name, module) in enumerate(model.named_children()):
        if hasattr(module, '__call__'):
            x = module(x)
            print(f"{i+1}. {name}: {x.shape}, mean={x.mean():.4f}, std={x.std():.4f}")
            
            # ì´ìƒ ê°’ ì²´í¬
            if torch.isnan(x).any():
                print(f"  âš ï¸ {name}ì—ì„œ NaN ë°œìƒ!")
                break
            if torch.isinf(x).any():
                print(f"  âš ï¸ {name}ì—ì„œ Inf ë°œìƒ!")
                break
            if x.std() < 1e-6:
                print(f"  âš ï¸ {name}ì—ì„œ ì¶œë ¥ì´ ê±°ì˜ ìƒìˆ˜!")
    
    return x

def comprehensive_model_diagnosis(model, sample_input, sample_target):
    """ì¢…í•©ì ì¸ ëª¨ë¸ ì§„ë‹¨"""
    print("=" * 50)
    print("ëª¨ë¸ ì¢…í•© ì§„ë‹¨ ì‹œì‘")
    print("=" * 50)
    
    # 1. ëª¨ë¸ êµ¬ì¡° í™•ì¸
    print("1. ëª¨ë¸ êµ¬ì¡°:")
    print(model)
    
    # 2. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í™•ì¸
    check_weight_initialization(model)
    
    # 3. Forward pass í…ŒìŠ¤íŠ¸
    output = debug_forward_pass(model, sample_input)
    
    # 4. Loss ê³„ì‚° ë° ê·¸ë˜ë””ì–¸íŠ¸ í™•ì¸
    if sample_target is not None:
        criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)
        try:
            loss = criterion(output.log_softmax(dim=-1).permute(1, 0, 2), 
                           sample_target, 
                           torch.tensor([output.size(1)]), 
                           torch.tensor([len(sample_target)]))
            print(f"Loss ê³„ì‚° ì„±ê³µ: {loss.item():.4f}")
            check_gradient_flow(model, loss)
        except Exception as e:
            print(f"âš ï¸ Loss ê³„ì‚° ì‹¤íŒ¨: {e}")
    
    print("=" * 50)
    print("ì§„ë‹¨ ì™„ë£Œ")
    print("=" * 50)

def setup(args):
    """Create configs and perform basic setups."""
    cfg = get_cfg()
    cfg.merge_from_file(args.config_file)
    cfg.merge_from_list(args.opts)
    cfg = default_setup(cfg, args)
    return cfg

####################################################################################################

def main(args):
    global best_wer
    logger = logging.getLogger()
    logging.basicConfig(filename="segment_ksl_log.log", level=logging.INFO)
    
    EPOCHS = 2000  # ì—í¬í¬ ìˆ˜ ì¦ê°€
    start_epoch = 0
    
    cfg = setup(args)
    cfg.GPU_ID = "cuda:0" if torch.cuda.is_available() else "cpu"
    os.environ['TZ'] = "ROK"
    time.tzset()
    cfg.OUTPUT_DIR = cfg.OUTPUT_DIR + time.strftime("_%Y-%m-%d_%H_%M_%S", time.localtime())
    cfg.freeze()
    # ì‹œê°„ êµ¬ê°„ë³„ ë°ì´í„°ë¡œë” ë¹Œë“œ
    train_loader, val_loader = build_data_loader(cfg)
    
    # CTC ì†ì‹¤ í•¨ìˆ˜ (zero_infinity=Trueë¡œ ë³€ê²½)
    loss_gls = nn.CTCLoss(blank=0, zero_infinity=True).to(cfg.GPU_ID)
    
    # í‚¤í¬ì¸íŠ¸ ì°¨ì›ì„ ë™ì ìœ¼ë¡œ ê³„ì‚°
    # OpenPose: ëª¸(25)+ì–¼êµ´(70)+ì–‘ì†(21*2) = 137ê°œ * 2ì°¨ì›(x,y) = 274
    input_dim = getattr(cfg, 'KEYPOINT_DIM', 274)
    num_classes = len(train_loader.dataset.vocab)
    
    print(f"ğŸ”§ ëª¨ë¸ ì„¤ì •: input_dim={input_dim}, num_classes={num_classes}")
    print(f"ğŸ“Š Train: {len(train_loader.dataset)}, Val: {len(val_loader.dataset)}")
    
    # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°œì„ 
    model = KeypointTransformer(
        num_classes=num_classes,
        input_dim=input_dim,
        d_model=512,  # ëª¨ë¸ ìš©ëŸ‰ ì¦ê°€
        nhead=8,
        num_encoder_layers=8,  # ë ˆì´ì–´ ìˆ˜ ì¦ê°€
        dim_feedforward=4096,  # FFN í¬ê¸° ì¦ê°€
        dropout=0.1
    )
    # sample_input = torch.randn(1, 100, 274).to(cfg.GPU_ID)
    # sample_target = torch.tensor([1, 2, 3])
    # comprehensive_model_diagnosis(model, sample_input, sample_target)
    # sys.exit(-1)
    model = model.to(cfg.GPU_ID)
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì • ê°œì„ 
    # optimizer = build_optimizer(cfg, model)
    # scheduler = build_lr_scheduler(cfg, optimizer)
    
    # # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ê°œì„  (ReduceLROnPlateau ì‚¬ìš©)
    # if not hasattr(scheduler, 'step') or 'MultiStep' in str(type(scheduler)):
    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    #         # optimizer, mode='min', factor=0.5, patience=8, verbose=True, min_lr=1e-7
    #         optimizer, mode='min', factor=0.6, patience=5, verbose=True, min_lr=1e-8, cooldown = 2, threshold = 5e-3, threshold_mode='rel'
    #     )
    #     print("ğŸ”„ ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ë³€ê²½")
    ####################################################################################################
    optimizer = Lion(
        model.parameters(),
        lr=1e-3,              # max_lrê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
        betas=(0.9, 0.99),    # Lion ê¸°ë³¸ê°’
        weight_decay=1e-2     # AdamWì˜ 10ë°°
    )
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=1e-3,                    # ì‹¤ì œ í•µì‹¬ í•™ìŠµë¥ 
        steps_per_epoch=len(train_loader),
        epochs=200,                      # ìˆ˜ì–´ ì¸ì‹ ê¶Œì¥ ì—í¬í¬
        pct_start=0.3,                  # 30% ì›Œë°ì—…
        div_factor=10,                  # ì‹œì‘ LR = 1e-3/25 = 4e-5
        final_div_factor=1000,         # ìµœì¢… LR = 1e-3/10000 = 1e-7
        anneal_strategy='cos'           # ì½”ì‚¬ì¸ ì–´ë‹ë§
    )
    ####################################################################################################
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
    if cfg.RESUME:
        assert os.path.isfile(cfg.RESUME), "Error: no checkpoint directory found!"
        checkpoint = torch.load(cfg.RESUME, map_location=cfg.GPU_ID, weights_only=False)
        best_wer = checkpoint['best_wer']
        start_epoch = checkpoint["epoch"]
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        if 'scheduler' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler'])
        logger.info(f"Loaded checkpoint from {cfg.RESUME}. Resuming at epoch {start_epoch}.")
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: epoch {start_epoch}, best WER: {best_wer:.3f}")
    
    # ê²€ì¦ë§Œ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
    if args.eval_only:
        metrics = validate(cfg, model, val_loader, loss_gls, start_epoch)
        print(f"Validation loss: {metrics['loss']:.3f}, Validation WER: {metrics['wer']:.3f}")
        return
    
    # TensorBoard ì„¤ì •
    writer = SummaryWriter(cfg.OUTPUT_DIR)
    data_time_meter = AverageMeter()
    loss_meter = AverageMeter()
    iter_time_meter = AverageMeter()
    epoch_time_meter = AverageMeter()

    ####################################################################################################
    
    # í•™ìŠµ ë£¨í”„
    last_best_idx = 0
    for epoch in range(start_epoch, EPOCHS):
        train_pbar = tqdm(train_loader, desc = f"Epoch {epoch + 1} / Train", mininterval=1)
        model.train()
        epoch_start = time.perf_counter()
        print(f"ğŸš€ Epoch {epoch + 1}/{EPOCHS} - Segment-based KSL Training")
        print(f"ğŸ“Š Best WER so far: {best_wer:.3f}")
        
        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        data_time_meter.reset()
        loss_meter.reset()
        iter_time_meter.reset()
        # for _iter, batch in enumerate(train_loader):
        for _iter, batch in enumerate(train_pbar):
            torch.cuda.empty_cache()
            start = time.perf_counter()
            
            (keypoints, keypoint_lengths), (glosses, gloss_lengths) = batch

            # ë¹ˆ ì‹œí€€ìŠ¤ í•„í„°ë§ (ê°œì„ )
            valid_indices = (keypoint_lengths > 0) & (gloss_lengths > 0)
            if not valid_indices.any():
                print(f"âš ï¸  ë°°ì¹˜ {_iter}: ìœ íš¨í•œ ì‹œí€€ìŠ¤ê°€ ì—†ìŒ, ê±´ë„ˆëœ€")
                continue
            
            keypoints = keypoints[valid_indices]
            keypoint_lengths = keypoint_lengths[valid_indices]
            glosses = glosses[valid_indices]
            gloss_lengths = gloss_lengths[valid_indices]
            
            data_time = time.perf_counter() - start
            data_time_meter.update(data_time, n=keypoints.size(0))
            
            # GPUë¡œ ì´ë™
            keypoints = keypoints.to(cfg.GPU_ID, non_blocking=True)
            glosses = glosses.to(cfg.GPU_ID, non_blocking=True)
            keypoint_lengths = keypoint_lengths.to(cfg.GPU_ID, non_blocking=True)
            gloss_lengths = gloss_lengths.to(cfg.GPU_ID, non_blocking=True)
            
            # NaN ì²˜ë¦¬ ì¶”ê°€
            keypoints = torch.nan_to_num(keypoints, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # Transformerë¥¼ ìœ„í•œ íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„±
            max_len = keypoints.size(1)
            src_key_padding_mask = torch.arange(max_len, device=keypoints.device)[None, :] >= keypoint_lengths[:, None]
            
            optimizer.zero_grad()
            
            try:
                # ëª¨ë¸ ì¶”ë¡ 
                gloss_scores = model(keypoints, src_key_padding_mask=src_key_padding_mask)

                # CTC ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•œ í˜•íƒœ ë³€í™˜
                gloss_probs = F.log_softmax(gloss_scores, dim=-1).permute(1, 0, 2)  # (T, N, C)

                # CTC ì†ì‹¤ ê³„ì‚°
                loss = loss_gls(gloss_probs, glosses, keypoint_lengths.long(), gloss_lengths.long())
                ##########################
                # CTC Greedy Decoding ê°œì„ 
                predictions = torch.argmax(gloss_scores, dim=-1)
                predictions_cpu = predictions.cpu().numpy()
                all_hypotheses, all_references = [], []
                vocab = train_loader.dataset.vocab
                # print(f"vocab : {vocab.itos}")
                # ë°°ì¹˜ë³„ ë””ì½”ë”©
                for b_idx in range(predictions_cpu.shape[0]):
                    pred_seq = predictions_cpu[b_idx][:keypoint_lengths[b_idx].cpu().item()]
                    # CTC ë””ì½”ë”©: blank(0)ì™€ ì—°ì† í† í° ì œê±°
                    decoded_indices = []
                    prev_token = -1
                    for token in pred_seq:
                        if token != 0 and token != prev_token:  # blankì™€ ì—°ì† í† í° ì œê±°
                            decoded_indices.append(token)
                        prev_token = token
                    
                    # ì˜ˆì¸¡ ë¬¸ì¥ ìƒì„±
                    if decoded_indices:
                        try:
                            hypothesis = vocab.arrays_to_sentences([decoded_indices])[0]
                            all_hypotheses.append(hypothesis)
                        except Exception as e:
                            print(f"âš ï¸  ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                            all_hypotheses.append([])
                    else:
                        all_hypotheses.append([])
                    
                    # ì°¸ì¡° ë¬¸ì¥ ìƒì„±
                    ref_seq = glosses[b_idx][:gloss_lengths[b_idx]].cpu().numpy()
                    reference = vocab.arrays_to_sentences([ref_seq])[0]
                    all_references.append(reference)

                # ì†ì‹¤ ê²€ì¦
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  Invalid loss detected at iteration {_iter}: {loss.item()}, skipping...")
                    continue
                
                loss_meter.update(loss.item(), n=keypoints.size(0))
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ë” ê°•í•˜ê²Œ)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                
                optimizer.step()
                
            except Exception as e:
                print(f"âŒ Error in training step {_iter}: {e}")
                continue
            
            iter_time = time.perf_counter() - start
            iter_time_meter.update(iter_time, n=keypoints.size(0))
            current_lr = optimizer.param_groups[0]['lr']
            train_pbar.set_postfix({
                'Loss': f'{loss_meter.avg:.4f}',
                'LR': f'{current_lr:.2e}'
            })
            # ë¡œê·¸ ì¶œë ¥
            if (_iter + 1) % cfg.PERIODS.LOG_ITERS == 0:
                """ current_lr = optimizer.param_groups[0]['lr']
                print(f"Epoch [{epoch+1}/{EPOCHS}][{_iter+1}/{len(train_loader)}] "
                      f"Loss: {loss_meter.avg:.4f}, "
                      f"LR: {current_lr:.2e}, "
                      f"Data Time: {data_time_meter.avg:.3f}s, "
                      f"Iter Time: {iter_time_meter.avg:.3f}s") """
                pass
                # TensorBoard ë¡œê¹…
            writer.add_scalar('train/loss', loss_meter.avg, epoch * len(train_loader) + _iter)
            writer.add_scalar('train/lr', current_lr, epoch * len(train_loader) + _iter)
        
        # ì—í¬í¬ë³„ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (MultiStep ë“±)
        if hasattr(scheduler, 'step') and 'ReduceLROnPlateau' not in str(type(scheduler)):
            scheduler.step()

        ####################################################################################################
        
        # ì—í¬í¬ ì‹œê°„ ì¸¡ì •
        epoch_time = time.perf_counter() - epoch_start
        epoch_time_meter.update(epoch_time, n=1)
        
        (f"â±ï¸  Epoch {epoch + 1} ì™„ë£Œ: {epoch_time:.1f}ì´ˆ ì†Œìš”")
        
        # ê²€ì¦ ì‹¤í–‰
        print(f"ğŸ” ê²€ì¦ ì‹œì‘...")
        metrics = validate(cfg, model, val_loader, loss_gls, epoch)

        ####################################################################################################
        
        # ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš© ì‹œ
        if hasattr(scheduler, 'step') and 'ReduceLROnPlateau' in str(type(scheduler)):
            scheduler.step(metrics['wer'])  # WER ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§
        
        ####################################################################################################
        
        # TensorBoard ë¡œê¹…
        for k, v in metrics.items():
            writer.add_scalar(f"val/{k}", v, epoch)
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info(f"Epoch {epoch + 1}: Val Loss={metrics['loss']:.3f}, Val WER={metrics['wer']:.3f}")
        print(f"âœ… Epoch {epoch + 1}: Val Loss={metrics['loss']:.3f}, Val WER={metrics['wer']:.3f}")
        
        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        is_best = metrics["wer"] < best_wer
        if is_best:
            print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! since {epoch - last_best_idx + 1} epoch. \nWER: {best_wer:.3f} â†’ {metrics['wer']:.3f}")
            best_wer = min(best_wer, metrics["wer"])
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_checkpoint({
                'epoch': epoch + 1,
                'state_dict': model.state_dict(),
                'wer': metrics["wer"],
                'best_wer': best_wer,
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
            }, is_best, cfg.OUTPUT_DIR, f"checkpoint.epoch_{epoch + 1}.tar")
            last_best_idx = epoch + 1
        print(f"Diff : {best_wer - metrics["wer"]}")
        
        
        # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (ì„ íƒì‚¬í•­)
        if best_wer < 5.0:  # WER 5% ì´í•˜ ë‹¬ì„± ì‹œ
            print(f"ğŸ¯ ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±! WER: {best_wer:.3f}% < 5.0%")
            break

    ####################################################################################################
    
    writer.close()
    print(f"ğŸ í•™ìŠµ ì™„ë£Œ! ìµœì¢… Best WER: {best_wer:.3f}")

####################################################################################################

def validate(cfg, model, val_loader, criterion, epoch) -> dict:
    """ê²€ì¦ í•¨ìˆ˜ - ì‹œê°„ êµ¬ê°„ë³„ ìˆ˜í™” ì¸ì‹ìš©ìœ¼ë¡œ ê°œì„ """
    logger = logging.getLogger()
    model.eval()
    val_loss_meter = AverageMeter()
    all_hypotheses, all_references = [], []
    vocab = val_loader.dataset.vocab
    
    print(f"ğŸ” ê²€ì¦ ì‹œì‘: {len(val_loader)} ë°°ì¹˜")
    val_pbar = tqdm(val_loader, desc = f"Epoch {epoch + 1} / Valid", mininterval=1)
    
    with torch.no_grad():
        # for batch_idx, batch in enumerate(val_loader):
        for batch_idx, batch in enumerate(val_pbar):
            torch.cuda.empty_cache()
            (keypoints, keypoint_lengths), (glosses, gloss_lengths) = batch
            
            # ë¹ˆ ì‹œí€€ìŠ¤ í•„í„°ë§
            valid_indices = (keypoint_lengths > 0) & (gloss_lengths > 0)
            if not valid_indices.any():
                continue
            
            keypoints = keypoints[valid_indices]
            keypoint_lengths = keypoint_lengths[valid_indices]
            glosses = glosses[valid_indices]
            gloss_lengths = gloss_lengths[valid_indices]
            
            # GPUë¡œ ì´ë™
            keypoints = keypoints.to(cfg.GPU_ID, non_blocking=True)
            glosses = glosses.to(cfg.GPU_ID, non_blocking=True)
            keypoint_lengths = keypoint_lengths.to(cfg.GPU_ID, non_blocking=True)
            gloss_lengths = gloss_lengths.to(cfg.GPU_ID, non_blocking=True)
            
            # NaN ì²˜ë¦¬
            keypoints = torch.nan_to_num(keypoints, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„±
            max_len = keypoints.size(1)
            src_key_padding_mask = torch.arange(max_len, device=keypoints.device)[None, :] >= keypoint_lengths[:, None]
            # print(f"valid ind : {valid_indices}")
            try:
                # ëª¨ë¸ ì¶”ë¡ 
                gloss_scores = model(keypoints, src_key_padding_mask=src_key_padding_mask)
                
                # ì†ì‹¤ ê³„ì‚°
                gloss_probs = F.log_softmax(gloss_scores, dim=-1).permute(1, 0, 2)
                loss = criterion(gloss_probs, glosses, keypoint_lengths.long(), gloss_lengths.long())
                
                if not (torch.isnan(loss) or torch.isinf(loss)):
                    val_loss_meter.update(loss.item(), n=keypoints.size(0))
                
                # CTC Greedy Decoding ê°œì„ 
                predictions = torch.argmax(gloss_scores, dim=-1)
                predictions_cpu = predictions.cpu().numpy()
                
                # ë°°ì¹˜ë³„ ë””ì½”ë”©
                for b_idx in range(predictions_cpu.shape[0]):
                    pred_seq = predictions_cpu[b_idx][:keypoint_lengths[b_idx].cpu().item()]
                    
                    # CTC ë””ì½”ë”©: blank(0)ì™€ ì—°ì† í† í° ì œê±°
                    decoded_indices = []
                    prev_token = -1
                    for token in pred_seq:
                        if token != 0 and token != prev_token:  # blankì™€ ì—°ì† í† í° ì œê±°
                            decoded_indices.append(token)
                        prev_token = token
                    
                    # ì˜ˆì¸¡ ë¬¸ì¥ ìƒì„±
                    if decoded_indices:
                        try:
                            # with val_pbar.external_write_mode():
                            #     print(f"decoded_indices : {decoded_indices}")
                            # val_pbar.refresh()
                            hypothesis = vocab.arrays_to_sentences([decoded_indices])[0]
                            all_hypotheses.append(hypothesis)
                        except Exception as e:
                            print(f"âš ï¸  ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                            all_hypotheses.append([])
                    else:
                        all_hypotheses.append([])
                    
                    # ì°¸ì¡° ë¬¸ì¥ ìƒì„±
                    ref_seq = glosses[b_idx][:gloss_lengths[b_idx]].cpu().numpy()
                    reference = vocab.arrays_to_sentences([ref_seq])[0]
                    all_references.append(reference)
                    # print(f"ref_seq : {ref_seq}, reference : {reference}")
                    # print(f"ref_seq : {ref_seq}, reference : {reference}, all_hyp : {list(filter(lambda x : x, all_hypotheses))}")
                
            except Exception as e:
                print(f"âŒ Validation error at batch {batch_idx}: {e}")
                continue
            
            # # ì§„í–‰ë¥  ì¶œë ¥
            # if (batch_idx + 1) % 50 == 0:
            #     print(f"ê²€ì¦ ì§„í–‰ë¥ : {batch_idx + 1}/{len(val_loader)}")
    
    # ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    print(f"\nğŸ“ ê²€ì¦ ê²°ê³¼ ìƒ˜í”Œ (ì´ {len(all_hypotheses)}ê°œ):")
    for i in range(min(5, len(all_hypotheses) // 5)):
        hyp_str = " ".join(all_hypotheses[i * 5]) if all_hypotheses[i * 5] else "<EMPTY>"
        ref_str = " ".join(all_references[i * 5]) if all_references[i * 5] else "<EMPTY>"
        print(f"  Sample {i+1}. Real : '{ref_str}, Pred: '{hyp_str}'")
        print(f"'")
    
    # WER ê³„ì‚°
    if all_hypotheses and all_references:
        # í…ìŠ¤íŠ¸ ì •ì œ
        gls_ref = [clean_ksl(" ".join(ref)) if ref else "" for ref in all_references]
        gls_hyp = [clean_ksl(" ".join(hyp)) if hyp else "" for hyp in all_hypotheses]
        
        # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        valid_pairs = [(h, r) for h, r in zip(gls_hyp, gls_ref) if r.strip()]
        if valid_pairs:
            gls_hyp, gls_ref = zip(*valid_pairs)
            metrics = wer_list(hypotheses=list(gls_hyp), references=list(gls_ref))
        else:
            print("âš ï¸  ìœ íš¨í•œ ì°¸ì¡° ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            metrics = {"wer": 100.0}
            raise ValueError("ìœ íš¨í•œ ì°¸ì¡° ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸  ì˜ˆì¸¡ ë˜ëŠ” ì°¸ì¡° ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        metrics = {"wer": 100.0}
    
    metrics.update({"loss": val_loss_meter.avg})
    
    print(f"ğŸ“Š ê²€ì¦ ì™„ë£Œ: Loss={metrics['loss']:.4f}, WER={metrics['wer']:.3f}%")
    return metrics

####################################################################################################

def save_checkpoint(state_dict, is_best, checkpoint_dir, filename='checkpoint.pth.tar'):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    filepath = os.path.join(checkpoint_dir, filename)
    torch.save(state_dict, filepath)
    
    if is_best:
        best_path = os.path.join(checkpoint_dir, 'model_best.pth.tar')
        shutil.copyfile(filepath, best_path)
        print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path}")

####################################################################################################

if __name__ == "__main__":
    mp.set_start_method('spawn', force=True)
    args = default_argument_parser().parse_args()
    args.config_file = '/home/azureuser/abcd/test/NIA_CSLR/configs/config.yaml'
    
    print("ğŸš€ ì‹œê°„ êµ¬ê°„ë³„ ìˆ˜í™” ì¸ì‹ í•™ìŠµ ì‹œì‘")
    print(f"ğŸ“ ì„¤ì • íŒŒì¼: {args.config_file}")

    main(args)