import os
import json
import pickle
from collections import defaultdict
from typing import List, Optional, Dict
from tqdm import tqdm

class GlossVocabulary:
    def __init__(self, tokens: Optional[List[str]] = None):
        self.PAD_TOKEN = "<PAD>"
        self.SOS_TOKEN = "<SOS>"
        self.EOS_TOKEN = "<EOS>"
        self.UNK_TOKEN = "<UNK>"
        self.SIL_TOKEN = "<SIL>"  # Silence token for future use
        
        self.specials = [self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN, self.UNK_TOKEN, self.SIL_TOKEN]
    
        self.itos = []
        self.stoi = defaultdict(self.unk_token)
        
        self.pad_idx = 0
        self.sos_idx = 1
        self.eos_idx = 2
        self.unk_idx = 3
        self.sil_idx = 4
        
        # ÌäπÏàò ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä
        for token in self.specials:
            self.itos.append(token)
            self.stoi[token] = len(self.itos) - 1
        
        # ÏùºÎ∞ò ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä
        if tokens:
            for token in tokens:
                if token not in self.specials:
                    self.add_token(token)
    
    def unk_token(self):
        return self.unk_idx
    
    def add_token(self, token: str) -> int:
        """ÌÜ†ÌÅ∞ÏùÑ Ïñ¥ÌúòÏÇ¨Ï†ÑÏóê Ï∂îÍ∞ÄÌïòÍ≥† Ïù∏Îç±Ïä§ Î∞òÌôò"""
        if token not in self.stoi:
            self.itos.append(token)
            self.stoi[token] = len(self.itos) - 1
        return self.stoi[token]
    
    def add_tokens(self, tokens: List[str]) -> List[int]:
        """Ïó¨Îü¨ ÌÜ†ÌÅ∞ÏùÑ ÌïúÎ≤àÏóê Ï∂îÍ∞Ä"""
        return [self.add_token(token) for token in tokens]
    
    def expand_vocab(self, new_tokens: List[str]):
        """Ïñ¥ÌúòÏÇ¨Ï†Ñ ÌôïÏû•"""
        print(f"Expanding vocabulary with {len(new_tokens)} new tokens...")
        added_count = 0
        for token in new_tokens:
            if token not in self.stoi:
                self.add_token(token)
                added_count += 1
        print(f"Added {added_count} new tokens. Vocabulary size: {len(self)}")
    
    def arrays_to_sentences(self, arrays: List[List[int]]) -> List[List[str]]:
        sentences = []
        for array in arrays:
            sentence = []
            for idx in array:
                if 0 <= idx < len(self.itos):
                    token = self.itos[idx]
                    if token not in [self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN]:
                        sentence.append(token)
                else:
                    # Ïù∏Îç±Ïä§Í∞Ä Î≤îÏúÑÎ•º Î≤óÏñ¥ÎÇòÎ©¥ UNK ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä
                    sentence.append(self.UNK_TOKEN)
                    print(f"‚ö†Ô∏è Invalid token index {idx} (vocab size: {len(self.itos)})")
            sentences.append(sentence)
        return sentences
    
    def save(self, filepath: str):
        """Ïñ¥ÌúòÏÇ¨Ï†ÑÏùÑ ÌååÏùºÎ°ú Ï†ÄÏû•"""
        vocab_data = {
            'itos': self.itos,
            'stoi': dict(self.stoi),
            'special_indices': {
                'pad_idx': self.pad_idx,
                'sos_idx': self.sos_idx,
                'eos_idx': self.eos_idx,
                'unk_idx': self.unk_idx,
                'sil_idx': self.sil_idx
            }
        }
        
        if filepath.endswith('.json'):
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        else:
            with open(filepath, 'wb') as f:
                pickle.dump(vocab_data, f)
        
        print(f"Vocabulary saved to {filepath}")
    
    @classmethod
    def load(cls, filepath: str):
        """ÌååÏùºÎ°úÎ∂ÄÌÑ∞ Ïñ¥ÌúòÏÇ¨Ï†Ñ Î°úÎìú"""
        if filepath.endswith('.json'):
            with open(filepath, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
        else:
            with open(filepath, 'rb') as f:
                vocab_data = pickle.load(f)
        
        vocab = cls()
        vocab.itos = vocab_data['itos']
        vocab.stoi = defaultdict(vocab.unk_token)
        vocab.stoi.update(vocab_data['stoi'])
        
        # ÌäπÏàò Ïù∏Îç±Ïä§ Î≥µÏõê
        special_indices = vocab_data.get('special_indices', {})
        vocab.pad_idx = special_indices.get('pad_idx', 0)
        vocab.sos_idx = special_indices.get('sos_idx', 1)
        vocab.eos_idx = special_indices.get('eos_idx', 2)
        vocab.unk_idx = special_indices.get('unk_idx', 3)
        vocab.sil_idx = special_indices.get('sil_idx', 4)
        
        print(f"Vocabulary loaded from {filepath}. Size: {len(vocab)}")
        return vocab

    def __len__(self):
        return len(self.itos)


def build_vocab_from_morpheme_dir(morpheme_dir: str, direction_filter: Optional[str] = None) -> GlossVocabulary:
    """morpheme ÎîîÎ†âÌÜ†Î¶¨Ïùò Î™®Îì† ÌååÏùºÎ°úÎ∂ÄÌÑ∞ Ïñ¥Ìúò ÏÇ¨Ï†Ñ Íµ¨Ï∂ï"""
    all_tokens = []
    processed_base_names = set()  # Ï§ëÎ≥µ Ï†úÍ±∞Î•º ÏúÑÌïú Í∏∞Î≥∏ Ïù¥Î¶Ñ Ï∂îÏ†Å
    
    # Find all morpheme files recursively
    morpheme_files = []
    for root, dirs, files in os.walk(morpheme_dir):
        for file in files:
            if file.endswith('_morpheme.json'):
                file_path = os.path.join(root, file)
                morpheme_files.append((file_path, file))
    
    for file_path, filename in tqdm(morpheme_files, desc="Building vocab"):
        # Î∞©Ìñ• ÌïÑÌÑ∞ÎßÅ (F, L, R, U, D Ï§ë ÌïòÎÇòÎßå Ï≤òÎ¶¨)
        if direction_filter:
            if not any(f"_{direction}_" in filename for direction in ['F', 'L', 'R', 'U', 'D']):
                continue
        
        # Í∏∞Î≥∏ Ïù¥Î¶Ñ Ï∂îÏ∂ú (Î∞©Ìñ• Ï†ïÎ≥¥ Ï†úÍ±∞)
        base_name = filename
        for direction in ['_F_', '_L_', '_R_', '_U_', '_D_']:
            if direction in base_name:
                base_name = base_name.replace(direction, '_X_')
                break
        
        # Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Í∏∞Î≥∏ Ïù¥Î¶ÑÏù¥Î©¥ Ïä§ÌÇµ
        if base_name in processed_base_names:
            continue
        processed_base_names.add(base_name)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            morpheme_data = json.load(f)
        
        # data Î∞∞Ïó¥ÏóêÏÑú morpheme Ï∂îÏ∂ú
        for item in morpheme_data.get('data', []):
            attributes = item.get('attributes', [])
            if attributes and len(attributes) > 0:
                morpheme_name = attributes[0].get('name', '')
                if morpheme_name:
                    all_tokens.append(morpheme_name)
    
    unique_tokens = list(set(all_tokens))
    vocab = GlossVocabulary(tokens=unique_tokens)
    print(f"üìö Ïñ¥Ìúò ÏÇ¨Ï†Ñ Íµ¨Ï∂ï ÏôÑÎ£å: {len(vocab)} ÌÜ†ÌÅ∞")
    return vocab


def load_or_build_vocab(vocab_path: str, morpheme_dir: str, force_rebuild: bool = False) -> GlossVocabulary:
    """Ïñ¥ÌúòÏÇ¨Ï†Ñ Î°úÎìú ÎòêÎäî ÎπåÎìú"""
    if os.path.exists(vocab_path) and not force_rebuild:
        print(f"Loading existing vocabulary from {vocab_path}")
        return GlossVocabulary.load(vocab_path)
    else:
        print(f"Building new vocabulary from {morpheme_dir}")
        vocab = build_vocab_from_morpheme_dir(morpheme_dir)
        vocab.save(vocab_path)
        return vocab